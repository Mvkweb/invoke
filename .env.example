# -----------------------------------------------------------------------------
# Environment Variables for Invoke Project
#
# This file serves as an example for your .env configuration.
# Copy this file to a new file named `.env` in the project root
# and fill in your actual API keys and desired settings.
#
# DO NOT commit your actual .env file to version control!
# -----------------------------------------------------------------------------

# --- AI Service Configuration ü§ñ ---
# API keys and model mappings for AI providers.

# Google Gemini API Key (Required): Get from https://aistudio.google.com/app/apikey
GEMINI_API_KEY="YOUR_GEMINI_API_KEY_HERE"

# OpenRouter API Key (Optional): Get from https://openrouter.ai/keys
# OPENROUTER_API_KEY="YOUR_OPENROUTER_API_KEY_HERE"

# AI Model Mappings:
# BIG_MODEL for powerful tasks (e.g., Sonnet, Opus)
# SMALL_MODEL for fast, cost-effective tasks (e.g., Haiku, Flash)
BIG_MODEL='gemini-2.0-flash-lite'
SMALL_MODEL='gemini-2.0-flash-lite'

# --- Server & Application Settings ‚öôÔ∏è ---
# Network interface, port, and logging level.
HOST="0.0.0.0"
PORT="8082"
LOG_LEVEL="WARNING"  # Options: DEBUG, INFO, WARNING, ERROR, CRITICAL

# --- Advanced Settings üöÄ ---
# Fine-tune performance, reliability, and streaming behavior.

# Max tokens allowed for AI model responses
MAX_TOKENS_LIMIT="8192"
# Timeout for API requests in seconds
REQUEST_TIMEOUT="90"
# Number of retries for failed API calls (LiteLLM specific for Gemini)
MAX_RETRIES="2"
# Specific retry attempts for streaming API calls
MAX_STREAMING_RETRIES="3"

# Streaming Control:
# Set to "true" to disable streaming globally
FORCE_DISABLE_STREAMING="false"
# Set to "true" for emergency streaming disable
EMERGENCY_DISABLE_STREAMING="false"